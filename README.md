## ğŸ”¬ Word Embeddings Using Latent Semantic Analysis (LSA)

### ğŸ“˜ Overview
This project implements **word embeddings using Latent Semantic Analysis (LSA)**.  
It constructs a **termâ€“document matrix** from text data, applies **Singular Value Decomposition (SVD)**, and produces semantic word vectors based on latent structure in the corpus.

ğŸ”— **Repository:** [techbeast0/Word-Embeddings-Using-Latent-Semantic-Analysis](https://github.com/techbeast0/Word-Embeddings-Using-Latent-Semantic-Analysis)

---

### âœ¨ Key Features
- Classical semantic embedding technique (non-neural)
- Termâ€“Document matrix construction (TF / TF-IDF)
- Dimensionality reduction with **SVD**
- Generates word vectors for similarity & clustering
- Implemented in a clear, reproducible **Jupyter Notebook**

---

### ğŸ“ Project Contents
- `WordEmbeddings.ipynb` â€” main notebook implementing LSA  
- Text preprocessing, matrix construction, and SVD walkthrough  
- Examples of cosine similarity and word-vector operations  

---

### ğŸ§  Use Cases
- Semantic similarity analysis  
- Document retrieval  
- Clustering and topic exploration  
- Educational use for understanding LSA fundamentals  

---

### ğŸ›  Tech Stack
- Python  
- NumPy / SciPy  
- Jupyter Notebook  
- (Optional) pandas, scikit-learn  

---

If you're exploring classical NLP methods or want lightweight embeddings without neural models, this project is a great starting point. ğŸš€
